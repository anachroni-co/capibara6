#!/usr/bin/env python3
"""
Configuraci√≥n de Modelos - Capibara6 Consensus
Soporte para m√∫ltiples modelos con diferentes configuraciones
"""

import os
from typing import Dict, List, Any

# ============================================
# CONFIGURACI√ìN DE MODELOS
# ============================================

MODELS_CONFIG = {
    'gpt-oss-20b': {
        'name': 'GPT-OSS-20B',
        'base_model': 'GPT-OSS-20B',
        'server_url': 'http://34.12.166.76:8000/v1',  # vLLM endpoint
        'type': 'vllm',
        'hardware': 'GPU',
        'status': 'active',
        'priority': 2,
        'prompt_template': {
            'system': 'Eres un asistente experto en programaci√≥n y an√°lisis t√©cnico.',
            'user': '{prompt}',
            'assistant': '',
            'stop_tokens': ['<|end|>', '']
        },
        'parameters': {
            'n_predict': 200,
            'temperature': 0.7,
            'top_p': 0.9,
            'repeat_penalty': 1.2,
            'stream': True
        }
    },

    'phi4': {
        'name': 'Phi-4 Mini',
        'base_model': 'Microsoft Phi-4 Mini (14B)',  # Upgraded to Phi-4 with more parameters
        'server_url': 'http://34.12.166.76:8001/v1',  # vLLM endpoint for Phi-4
        'type': 'vllm',
        'hardware': 'GPU',
        'status': 'active',
        'priority': 3,
        'prompt_template': {
            'system': 'You are a helpful AI assistant. Respond concisely and accurately.',
            'user': '{prompt}',
            'assistant': '',
            'stop_tokens': ['<|end|>', '']
        },
        'parameters': {
            'n_predict': 120,  # Updated for longer context
            'temperature': 0.5,
            'top_p': 0.85,
            'repeat_penalty': 1.2,
            'stream': True
        }
    },

    'qwen2.5-coder': {
        'name': 'Qwen2.5-Coder 1.5B',
        'base_model': 'Qwen/Qwen2.5-Coder-1.5B-Instruct',
        'server_url': 'http://34.12.166.76:8002/v1',  # vLLM endpoint for the code model
        'type': 'vllm',
        'hardware': 'GPU',
        'status': 'active',
        'priority': 2,
        'prompt_template': {
            'system': 'You are an expert code assistant. Provide accurate, efficient, and well-documented code solutions.',
            'user': '{prompt}',
            'assistant': '',
            'stop_tokens': ['<|end|>', '']
        },
        'parameters': {
            'n_predict': 200,  # Suitable for code tasks
            'temperature': 0.3,
            'top_p': 0.9,
            'repeat_penalty': 1.1,
            'stream': True
        }
    },

    'mixtral': {
        'name': 'Mixtral 8x7B',
        'base_model': 'Mixtral-8x7B-Instruct-v0.1',
        'server_url': 'http://34.12.166.76:8003/v1',  # vLLM endpoint for Mixtral
        'type': 'vllm',
        'hardware': 'GPU',
        'status': 'active',
        'priority': 2,
        'prompt_template': {
            'system': 'You are a creative and multilingual AI assistant. Provide detailed and engaging responses.',
            'user': '[INST] {prompt} [/INST]',
            'assistant': '',
            'stop_tokens': ['</s>', '[/INST]', '']
        },
        'parameters': {
            'n_predict': 250,
            'temperature': 0.7,
            'top_p': 0.95,
            'repeat_penalty': 1.1,
            'stream': True
        }
    }
}

# ============================================
# PLANTILLAS DE PROMPTS POR CATEGOR√çA
# ============================================

PROMPT_TEMPLATES = {
    'general': {
        'name': 'General',
        'description': 'Conversaci√≥n general y preguntas abiertas',
        'system_prompt': 'Eres un asistente √∫til y preciso. Responde de manera clara y concisa.',
        'models': ['phi4', 'qwen2.5-coder'],
        'requires_execution': False,
        'execution_context': 'none'
    },

    'coding': {
        'name': 'Programaci√≥n',
        'description': 'Ayuda con c√≥digo, debugging y desarrollo',
        'system_prompt': 'Eres un experto programador. Proporciona c√≥digo limpio, bien documentado y con ejemplos. Si se requiere ejecuci√≥n real del c√≥digo, prepara c√≥digo que pueda ser ejecutado en un entorno seguro de E2B.',
        'models': ['qwen2.5-coder', 'gpt-oss-20b'],
        'additional_instructions': 'Siempre usa bloques de c√≥digo markdown con el lenguaje especificado. Detecta cu√°ndo se necesita ejecuci√≥n real del c√≥digo.',
        'requires_execution': True,
        'execution_context': 'e2b_python'
    },

    'analysis': {
        'name': 'An√°lisis',
        'description': 'An√°lisis de datos, investigaci√≥n y pensamiento cr√≠tico',
        'system_prompt': 'Eres un analista experto. Proporciona an√°lisis estructurado, evidencia y conclusiones claras. Si se proporcionan datos para analizar, genera c√≥digo que pueda ejecutarse en entorno E2B para procesamiento real.',
        'models': ['gpt-oss-20b', 'mixtral'],  # Mejores para an√°lisis complejos
        'additional_instructions': 'Estructura tu respuesta con: 1) Resumen, 2) An√°lisis detallado, 3) Conclusiones. Genera c√≥digo para an√°lisis de datos cuando sea relevante.',
        'requires_execution': True,
        'execution_context': 'e2b_data_analysis'
    },

    'creative': {
        'name': 'Creativo',
        'description': 'Escritura creativa, storytelling y contenido',
        'system_prompt': 'Eres un escritor creativo y original. Crea contenido atractivo y bien estructurado.',
        'models': ['mixtral', 'gpt-oss-20b'],
        'additional_instructions': 'Usa un tono apropiado para el contexto y mant√©n la coherencia narrativa.',
        'requires_execution': False,
        'execution_context': 'none'
    },

    'technical': {
        'name': 'T√©cnico',
        'description': 'Documentaci√≥n t√©cnica, arquitectura y sistemas',
        'system_prompt': 'Eres un arquitecto de software experto. Proporciona documentaci√≥n t√©cnica precisa y detallada. Genera c√≥digo de ejemplo que pueda ser ejecutado para verificar funcionalidad.',
        'models': ['gpt-oss-20b', 'qwen2.5-coder'],  # Mejores para documentaci√≥n t√©cnica
        'additional_instructions': 'Incluye diagramas en formato Mermaid cuando sea apropiado. Proporciona c√≥digo de ejemplo que pueda ejecutarse en entorno E2B.',
        'requires_execution': True,
        'execution_context': 'e2b_code_example'
    }
}

# ============================================
# CONFIGURACI√ìN DE CONSENSO
# ============================================

CONSENSUS_CONFIG = {
    'enabled': True,
    'min_models': 2,
    'max_models': 3,
    'voting_method': 'weighted',  # 'simple', 'weighted', 'confidence'
    'model_weights': {
        'phi4': 0.7,      # Modelo r√°pido y eficiente
        'qwen2.5-coder': 0.8,  # Modelo experto en c√≥digo
        'gpt-oss-20b': 0.9,    # Modelo m√°s potente
        'mixtral': 0.6      # Buen modelo general
    },
    'fallback_model': 'phi4',  # Modelo de respaldo si falla el consenso
    'timeout': 30  # Segundos para esperar respuestas
}

# ============================================
# FUNCIONES DE UTILIDAD
# ============================================

def get_active_models() -> List[str]:
    """Obtiene la lista de modelos activos"""
    return [model_id for model_id, config in MODELS_CONFIG.items()
            if config['status'] == 'active']

def get_model_config(model_id: str) -> Dict[str, Any]:
    """Obtiene la configuraci√≥n de un modelo espec√≠fico"""
    return MODELS_CONFIG.get(model_id, {})

def get_prompt_template(template_id: str) -> Dict[str, Any]:
    """Obtiene una plantilla de prompt espec√≠fica"""
    return PROMPT_TEMPLATES.get(template_id, {})

def get_available_templates() -> List[str]:
    """Obtiene la lista de plantillas disponibles"""
    return list(PROMPT_TEMPLATES.keys())

def get_models_for_template(template_id: str) -> List[str]:
    """Obtiene los modelos recomendados para una plantilla"""
    template = get_prompt_template(template_id)
    return template.get('models', [])

def format_prompt(model_id: str, template_id: str, user_prompt: str) -> str:
    """Formatea un prompt usando la plantilla y configuraci√≥n del modelo"""
    model_config = get_model_config(model_id)
    template = get_prompt_template(template_id)

    if not model_config or not template:
        return user_prompt

    # Obtener el template del modelo
    model_template = model_config.get('prompt_template', {})
    system_prompt = template.get('system_prompt', model_template.get('system', ''))

    # Verificar si se requiere ejecuci√≥n de c√≥digo
    requires_execution = template.get('requires_execution', False)

    if requires_execution:
        execution_context = template.get('execution_context', 'e2b_python')
        execution_instructions = f"\n\nNOTA IMPORTANTE: Esta consulta requiere ejecuci√≥n de c√≥digo. El c√≥digo resultante ser√° ejecutado en un entorno seguro E2B ({execution_context}). Aseg√∫rate de que el c√≥digo sea funcional y est√© listo para ejecuci√≥n."
        system_prompt += execution_instructions

    # Formatear seg√∫n el tipo de modelo
    if model_id in ['gpt-oss-20b', 'phi4', 'qwen2.5-coder']:
        # Formato est√°ndar para vLLM
        return f"{system_prompt}\n\n{user_prompt}"
    elif model_id == 'mixtral':
        # Formato espec√≠fico para Mixtral
        return f"[INST] {system_prompt} {user_prompt} [/INST]"

    return user_prompt

# ============================================
# CONFIGURACI√ìN DE DESARROLLO
# ============================================

def get_development_config():
    """Configuraci√≥n para desarrollo local"""
    return {
        'models': {
            'gpt-oss-20b': {
                **MODELS_CONFIG['gpt-oss-20b'],
                'server_url': 'http://localhost:8000/v1'
            },
            'phi4': {
                **MODELS_CONFIG['phi4'],
                'server_url': 'http://localhost:8001/v1'
            },
            'qwen2.5-coder': {
                **MODELS_CONFIG['qwen2.5-coder'],
                'server_url': 'http://localhost:8002/v1'
            },
            'mixtral': {
                **MODELS_CONFIG['mixtral'],
                'server_url': 'http://localhost:8003/v1'
            }
        },
        'consensus': CONSENSUS_CONFIG
    }

def get_production_config():
    """Configuraci√≥n para producci√≥n"""
    return {
        'models': MODELS_CONFIG,
        'consensus': CONSENSUS_CONFIG
    }

# ============================================
# INFORMACI√ìN DEL SISTEMA
# ============================================

def get_system_info():
    """Obtiene informaci√≥n del sistema de modelos"""
    active_models = get_active_models()
    return {
        'total_models': len(MODELS_CONFIG),
        'active_models': len(active_models),
        'models_list': active_models,
        'consensus_enabled': CONSENSUS_CONFIG['enabled'],
        'available_templates': get_available_templates(),
        'hardware_info': {
            model_id: config['hardware']
            for model_id, config in MODELS_CONFIG.items()
            if config['status'] == 'active'
        }
    }

if __name__ == '__main__':
    print("ü§ñ Configuraci√≥n de Modelos Capibara6")
    print("=" * 50)

    info = get_system_info()
    print(f"Modelos activos: {info['active_models']}/{info['total_models']}")
    print(f"Consenso habilitado: {info['consensus_enabled']}")
    print(f"Plantillas disponibles: {len(info['available_templates'])}")

    print("\nüìã Modelos configurados:")
    for model_id in info['models_list']:
        config = get_model_config(model_id)
        print(f"  ‚Ä¢ {config['name']} ({config['hardware']}) - {config['status']}")

    print("\nüéØ Plantillas disponibles:")
    for template_id in info['available_templates']:
        template = get_prompt_template(template_id)
        print(f"  ‚Ä¢ {template['name']}: {template['description']}")