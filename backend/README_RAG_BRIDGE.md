# üöÄ RAG Bridge - Quick Start Guide

## Resumen

Esta integraci√≥n permite que los modelos de Ollama en **bounty2** accedan al sistema RAG completo en **RAG3** para enriquecer sus respuestas con datos personales del usuario.

## Instalaci√≥n R√°pida en bounty2

```bash
cd /home/elect/capibara6/backend

# 1. Ejecutar script de configuraci√≥n
./setup_rag_bridge.sh

# 2. Verificar que todo funciona
python3 rag_client.py
python3 ollama_rag_integration.py
```

## Uso B√°sico

### Opci√≥n 1: Cliente Simple

```python
from rag_client import get_rag_context

# Obtener contexto RAG para una consulta
context = get_rag_context("¬øQu√© he hablado sobre IA?")

# Agregar contexto al prompt de Ollama
full_prompt = f"""
Informaci√≥n del usuario:
{context}

---

Usuario: ¬øQu√© he hablado sobre IA?
"""
```

### Opci√≥n 2: Integraci√≥n Completa (Recomendado)

```python
from ollama_rag_integration import create_integrated_client
import json

# Cargar configuraci√≥n
with open("../model_config.json") as f:
    ollama_config = json.load(f)

# Crear cliente integrado
client = create_integrated_client(ollama_config)

# Generar respuesta (usa RAG autom√°ticamente cuando es necesario)
response = client.generate_with_rag(
    prompt="¬øQu√© he comentado sobre machine learning?",
    model_tier="balanced"
)

print(f"Respuesta: {response['response']}")
print(f"RAG usado: {response['rag_used']}")
```

### Opci√≥n 3: Servidor Flask Completo

```bash
# Iniciar servidor de ejemplo
python3 example_rag_bridge_server.py

# En otro terminal, probar
curl -X POST http://localhost:5001/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "¬øQu√© he guardado sobre IA?"}'
```

## Arquitectura

```
bounty2 (Ollama)  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫  RAG3 (Sistema RAG)
     ‚îÇ                           ‚îÇ
     ‚îÇ    HTTP Request           ‚îÇ
     ‚îÇ    10.154.0.2:8000        ‚îÇ
     ‚îÇ                           ‚îÇ
     ‚îú‚îÄ rag_client.py           ‚îú‚îÄ Milvus (vectors)
     ‚îú‚îÄ ollama_rag_integration  ‚îú‚îÄ PostgreSQL (data)
     ‚îî‚îÄ example_server.py       ‚îî‚îÄ Nebula (graph)
```

## Endpoints del Servidor de Ejemplo

- `GET /` - Informaci√≥n del servidor
- `GET /api/health` - Health check (Ollama + RAG)
- `POST /api/chat` - Chat con integraci√≥n RAG
- `POST /api/chat/stream` - Chat con streaming
- `GET /api/rag/status` - Estado de conexi√≥n RAG
- `POST /api/rag/search` - B√∫squeda directa en RAG
- `GET /api/models` - Listar modelos Ollama

## Ejemplos de Uso

### Ejemplo 1: Chat Normal (Sin RAG)

```bash
curl -X POST http://localhost:5001/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "¬øQu√© es machine learning?",
    "model_tier": "fast_response"
  }'
```

Respuesta:
```json
{
  "response": "Machine learning es una rama de la inteligencia artificial...",
  "model": "phi3:mini",
  "rag_used": false,
  "metadata": {...}
}
```

### Ejemplo 2: Chat Personal (Con RAG)

```bash
curl -X POST http://localhost:5001/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "¬øQu√© he comentado sobre machine learning?",
    "model_tier": "balanced",
    "use_rag": true
  }'
```

Respuesta:
```json
{
  "response": "Bas√°ndome en tus conversaciones anteriores, has comentado sobre...",
  "model": "mistral:latest",
  "rag_used": true,
  "rag_metadata": {
    "confidence": 0.6,
    "context_length": 1234
  }
}
```

### Ejemplo 3: Streaming

```bash
curl -X POST http://localhost:5001/api/chat/stream \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Expl√≠came qu√© es RAG",
    "model_tier": "balanced"
  }'
```

### Ejemplo 4: B√∫squeda RAG Directa

```bash
curl -X POST http://localhost:5001/api/rag/search \
  -H "Content-Type: application/json" \
  -d '{
    "query": "IA y embeddings",
    "n_results": 5,
    "use_graph": true
  }'
```

## Configuraci√≥n Avanzada

### Variables de Entorno

```bash
# .env o .env.rag_bridge
RAG_API_URL=http://10.154.0.2:8000
OLLAMA_ENDPOINT=http://localhost:11434
DEFAULT_MODEL_TIER=balanced
RAG_THRESHOLD=0.3
RAG_CONTEXT_MAX_LENGTH=1500
RAG_TIMEOUT=30
PORT=5001
DEBUG=false
```

### Personalizar Detecci√≥n RAG

```python
from ollama_rag_integration import OllamaRAGIntegration

# Crear con umbral personalizado
integration = OllamaRAGIntegration(
    rag_threshold=0.4,  # Requiere m√°s confianza
    context_max_length=2000  # M√°s contexto
)

# Verificar si una consulta usar√≠a RAG
should_use, score = integration.should_use_rag("mi pregunta")
print(f"Usar RAG: {should_use}, Score: {score}")
```

### Agregar Patrones Personalizados

```python
# En ollama_rag_integration.py
OllamaRAGIntegration.RAG_TRIGGERS.extend([
    r"\b(proyecto|trabajo)\b",
    r"mis (notas|apuntes|documentos)"
])
```

## Monitoreo y Logs

```bash
# Ver logs del servidor
tail -f /var/log/capibara6/server.log

# Verificar conectividad
curl http://localhost:5001/api/health

# Verificar estado RAG
curl http://localhost:5001/api/rag/status
```

## Troubleshooting

### Error: "Connection refused to RAG3"

```bash
# Verificar conectividad
ping 10.154.0.2

# Verificar que RAG API est√° corriendo
curl http://10.154.0.2:8000/health

# Verificar firewall
gcloud compute firewall-rules list | grep rag3
```

### Error: "Ollama not responding"

```bash
# Verificar proceso de Ollama
ps aux | grep ollama

# Reiniciar Ollama
sudo systemctl restart ollama

# Verificar puerto
curl http://localhost:11434/api/tags
```

### RAG siempre retorna contexto vac√≠o

```python
# Reducir umbral de similitud
rag_client.search_rag(
    query="mi consulta",
    n_results=10,  # M√°s resultados
    use_graph=False  # Desactivar grafo si es lento
)
```

## Performance

- **Latencia VM-to-VM:** ~9ms (red privada GCloud)
- **B√∫squeda RAG:** 100-500ms (dependiendo de complejidad)
- **Generaci√≥n Ollama:** Variable seg√∫n modelo
  - phi3:mini: 50-200ms
  - mistral: 200-800ms
  - gpt-oss:20b: 1-5s

## Seguridad

- ‚úÖ Comunicaci√≥n por red privada de GCloud
- ‚úÖ Sin exposici√≥n a Internet
- ‚úÖ Firewall de GCloud
- ‚ö†Ô∏è Considera agregar autenticaci√≥n con API keys para producci√≥n

## M√°s Informaci√≥n

- **Documentaci√≥n completa:** `OLLAMA_RAG_BRIDGE.md`
- **Configuraci√≥n de VM:** `web/REAL_VM_SETUP.md`
- **API del sistema RAG:** http://10.154.0.2:8000/docs (si tienes FastAPI docs habilitado)

## Soporte

Si encuentras problemas:

1. Ejecuta el script de diagn√≥stico: `./setup_rag_bridge.sh`
2. Revisa los logs del servidor
3. Verifica la conectividad de red
4. Consulta la documentaci√≥n completa

---

*√öltima actualizaci√≥n: 2025-11-11*
