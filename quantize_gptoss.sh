#!/bin/bash
set -e

echo "=== ‚öôÔ∏è Cuantificaci√≥n INT8 de gpt-oss-120B ==="

MODEL_ORIG="/mnt/models/gpt-oss-120b/original"
MODEL_OUT="/mnt/models/gpt-oss-120b-int8"

# Activar entorno virtual (creado por startup.sh)
source /opt/venv/bin/activate

# Instalar dependencias necesarias
pip install -U auto-gptq transformers accelerate safetensors --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu122/

# Verificar existencia del modelo original
if [ ! -d "$MODEL_ORIG" ]; then
  echo "‚ùå No se encontr√≥ el modelo original en $MODEL_ORIG"
  exit 1
fi

# Crear carpeta destino
mkdir -p "$MODEL_OUT"

# Ejecutar cuantificaci√≥n
python - <<'PY'
from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig
from transformers import AutoTokenizer
import torch, os

model_name = "/mnt/models/gpt-oss-120b/original"
save_dir = "/mnt/models/gpt-oss-120b-int8"

print(f"üß© Cargando modelo desde {model_name}...")
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
quant_config = BaseQuantizeConfig(bits=8, group_size=128, desc_act=False)

model = AutoGPTQForCausalLM.from_pretrained(
    model_name,
    quantize_config=quant_config,
    torch_dtype=torch.float16,
    device_map="auto"
)

print("‚öôÔ∏è Iniciando cuantificaci√≥n a INT8...")
model.quantize(examples=["El mundo cambia con la inteligencia artificial."])

print(f"üíæ Guardando modelo cuantizado en {save_dir} ...")
model.save_quantized(save_dir)
tokenizer.save_pretrained(save_dir)

print("‚úÖ Cuantificaci√≥n completada correctamente.")
PY

# Mostrar tama√±o final
echo "üì¶ Tama√±o final del modelo cuantizado:"
du -sh "$MODEL_OUT"

echo "=== üü¢ Proceso de cuantificaci√≥n finalizado ==="
