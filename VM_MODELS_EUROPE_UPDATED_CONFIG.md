# ConfiguraciÃ³n Actualizada de la Arquitectura Capibara6

## ğŸ–¥ï¸ VM models-europe (34.175.48.2 / 10.204.0.9)

### Servicios Activos

#### 1. vLLM Multi-Model Server (Puerto 8080) - **PRINCIPAL**
**Arquitectura:**
- Sistema de consenso y routing de modelos
- Lazy loading automÃ¡tico (max 3 modelos cargados)
- Auto-unload despuÃ©s de 300s de inactividad
- Compatible con OpenAI API

**Modelos Disponibles:**

| Modelo | Estado | Dominio | DescripciÃ³n |
|--------|--------|---------|-------------|
| `phi4_fast` | âœ… Cargado | General | Respuestas rÃ¡pidas y simples |
| `mistral_balanced` | âœ… Cargado | Technical | Tareas tÃ©cnicas intermedias |
| `qwen_coder` | âœ… Cargado | Coding | Especializado en cÃ³digo |
| `gemma3_multimodal` | âœ… Cargado | Complex Reasoning | AnÃ¡lisis complejo y multimodal |
| `aya_expanse_multilingual` | âœ… Cargado | Multilingual | MultilingÃ¼e y razonamiento complejo |

**Endpoints:**

```bash
# Health & Stats
GET  /health              # Health check
GET  /stats               # EstadÃ­sticas y modelos cargados

# OpenAI Compatible
GET  /v1/models           # Lista de modelos
POST /v1/chat/completions # Chat completion
POST /v1/completions      # Text completion

# Ollama Compatible
POST /api/generate        # Generate text
```

**Ejemplo de uso:**

```bash
# Listar modelos
curl http://10.204.0.9:8080/v1/models

# Chat completion
curl http://10.204.0.9:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi4_fast",
    "messages": [{"role": "user", "content": "Hola"}]
  }'
```

#### 2. Ollama (Puerto 11434) - **ALTERNATIVO**

**Modelos:**
- `gpt-oss:20b` (13.8 GB, MXFP4)
- `mistral:latest` (4.4 GB, Q4_K_M)
- `phi3:mini` (2.2 GB, Q4_0)

**Uso:**
```bash
curl http://10.204.0.9:11434/api/generate \
  -d '{"model": "gpt-oss:20b", "prompt": "Hola"}'
```

### ConexiÃ³n con rag-europe

models-europe se conecta con rag-europe para:
- Consultar bases de datos vectoriales (Milvus)
- Buscar en grafos de conocimiento (Nebula)
- Recuperar contexto de PostgreSQL
- Cache en Redis

```
models-europe â†’ Bridge API (10.204.0.10:8000)
               â†’ /api/v1/rag/hybrid-search
               â†’ /api/v1/milvus/search
               â†’ /api/v1/nebula/query
```

---

## ğŸ”„ Flujo de Datos Actualizado

```
Usuario/Cliente
      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ services (10.204.0.5)                   â”‚
â”‚ â”œâ”€ Nginx (80/443) â†’ Proxy               â”‚
â”‚ â”œâ”€ Flask API (5000) â†’ Backend           â”‚
â”‚ â”œâ”€ TTS (5001/5002) â†’ Texto a voz        â”‚
â”‚ â”œâ”€ MCP (5003) â†’ Context Protocol        â”‚
â”‚ â””â”€ n8n (5678) â†’ Workflows               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“ (peticiones de IA)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ models-europe (10.204.0.9)              â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ vLLM Server (8080) - PRINCIPAL      â”‚ â”‚
â”‚ â”‚ â”œâ”€ Router/Consenso                  â”‚ â”‚
â”‚ â”‚ â”œâ”€ phi4_fast (rÃ¡pido)               â”‚ â”‚
â”‚ â”‚ â”œâ”€ mistral_balanced (balanceado)    â”‚ â”‚
â”‚ â”‚ â”œâ”€ qwen_coder (cÃ³digo)              â”‚ â”‚
â”‚ â”‚ â”œâ”€ gemma3_multimodal (anÃ¡lisis)     â”‚ â”‚
â”‚ â”‚ â””â”€ aya_expanse_multilingual (multi) â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ Ollama (11434) - Alternativo            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“ (necesita contexto/datos)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ rag-europe (10.204.0.10)                â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Bridge API (8000)                   â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚      â†“           â†“           â†“          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Milvus â”‚ â”‚ Nebula â”‚ â”‚Postgres â”‚     â”‚
â”‚  â”‚(vector)â”‚ â”‚(grafos)â”‚ â”‚  (SQL)  â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                â†“                        â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚          â”‚  Redis  â”‚                    â”‚
â”‚          â”‚ (cache) â”‚                    â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Variables de Entorno Actualizadas

### Backend

```bash
# vLLM Server (PRINCIPAL)
VLLM_URL=http://10.204.0.9:8080/v1/chat/completions
VLLM_COMPLETIONS_URL=http://10.204.0.9:8080/v1/completions
VLLM_MODELS_URL=http://10.204.0.9:8080/v1/models
VLLM_HEALTH_URL=http://10.204.0.9:8080/health

# Ollama (FALLBACK)
OLLAMA_URL=http://10.204.0.9:11434/api/generate
```

### Vercel (Frontend Proxy)

```bash
# vLLM Principal
VLLM_URL=http://34.175.48.2:8080/v1/chat/completions

# Ollama Fallback
OLLAMA_URL=http://34.175.48.2:11434/api/generate
```

---

## ğŸ“¡ Endpoints Actualizados

### vLLM (Puerto 8080)

| Endpoint | MÃ©todo | DescripciÃ³n |
|----------|--------|-------------|
| `/v1/models` | GET | Listar modelos disponibles |
| `/v1/chat/completions` | POST | Completions de chat (OpenAI compatible) |
| `/v1/completions` | POST | Completions de texto (OpenAI compatible) |
| `/health` | GET | Health check del servidor |
| `/stats` | GET | EstadÃ­sticas del servidor |

### Ollama (Puerto 11434)

| Endpoint | MÃ©todo | DescripciÃ³n |
|----------|--------|-------------|
| `/api/generate` | POST | GeneraciÃ³n de texto |
| `/api/chat` | POST | Chat completions |
| `/api/tags` | GET | Model tags |

---

## ğŸ§ª Pruebas de ConexiÃ³n

### Verificar vLLM:

```bash
# Health check
curl http://10.204.0.9:8080/health

# Listar modelos
curl http://10.204.0.9:8080/v1/models

# Test de completions
curl http://10.204.0.9:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi4_fast",
    "messages": [{"role": "user", "content": "Hola, Â¿cÃ³mo estÃ¡s?"}],
    "temperature": 0.7,
    "max_tokens": 100
  }'
```

### Verificar Ollama:

```bash
# Listar modelos
curl http://10.204.0.9:11434/api/tags

# Test de completions
curl http://10.204.0.9:11434/api/generate \
  -d '{"model": "gpt-oss:20b", "prompt": "Hola", "stream": false}'
```