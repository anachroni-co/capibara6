# Capibara6 - Sistema de IA con vLLM

## Descripci√≥n General

Capibara6 es una plataforma de Inteligencia Artificial conversacional que utiliza m√∫ltiples modelos de lenguaje para proporcionar respuestas inteligentes a trav√©s de un sistema de enrutamiento sem√°ntico y consenso. El sistema ha sido migrado de Ollama a vLLM con endpoints compatibles con OpenAI.

## Arquitectura del Sistema

### Componentes Principales

1. **Backend Principal (vm-bounty2)**
   - Sistema de m√∫ltiples modelos con enrutamiento inteligente
   - Sistema de consenso con votaci√≥n ponderada
   - Integraci√≥n con RAG y E2B
   
2. **Sistema RAG (vm-rag3)**
   - Milvus (vector database)
   - Nebula Graph (knowledge graph)
   - PostgreSQL (metadata relacional)
   
3. **Servicios Especializados (vm-services)**
   - MCP (Smart Model Controller Protocol)
   - Kyutai TTS (s√≠ntesis de voz)
   - Coqui XTTS v2 (mejora de calidad de voz)

### Modelos Activos

1. **phi4:mini** - Modelo r√°pido para tareas simples (14B params)
2. **qwen2.5-coder:1.5b** - Experto en programaci√≥n y tareas t√©cnicas
3. **gpt-oss-20b** - Modelo complejo para an√°lisis profundos
4. **mixtral** - Modelo general para tareas creativas

## Migraci√≥n a vLLM

### Cambios Importantes

- **Ollama API** ‚Üí **vLLM OpenAI-Compatible API**
  - Endpoints: `/api/generate` ‚Üí `/v1/chat/completions`
  - Formato: `prompt`-based ‚Üí `messages`-based (`{"role": "user", "content": "texto"}`)
  - Autenticaci√≥n: Opcional con "Bearer EMPTY"

- **Actualizaci√≥n de Modelos**
  - `phi3:mini` ‚Üí `phi4:mini` (de 3.8B a 14B par√°metros)
  - `mistral` ‚Üí `qwen2.5-coder:1.5b` (modelo experto en c√≥digo)

### Configuraci√≥n de Endpoints

- **vLLM Endpoint Principal**: `http://34.12.166.76:8000/v1`
- **phi4 Endpoint**: `http://34.12.166.76:8001/v1`
- **qwen2.5-coder Endpoint**: `http://34.12.166.76:8002/v1`
- **RAG3 Endpoint**: `http://10.154.0.2:8000/` (interno)

## Sistema de Consenso

### Configuraci√≥n

- **M√©todo**: Votaci√≥n ponderada ('weighted')
- **Pesos**: phi4 (0.7), qwen2.5-coder (0.8), gpt-oss-20b (0.9), mixtral (0.6)
- **Min/Max modelos**: 2/3 para consenso
- **Fallback**: phi4 como modelo de respaldo

## Integraci√≥n RAG-E2B-TOON

### Sistema RAG
- **MiniRAG**: B√∫squeda r√°pida y ligera
- **FullRAG**: B√∫squeda profunda con expansi√≥n de queries
- **Vector Store**: Basado en Milvus (VM RAG3)

### Sistema E2B
- **Integraci√≥n autom√°tica**: Detecta cu√°ndo se necesita ejecuci√≥n de c√≥digo
- **Ejecuci√≥n segura**: En entornos sandbox para c√≥digo propuesto

### Sistema TOON
- **Optimizaci√≥n de tokens**: Reducci√≥n de 30-60% en contexto RAG
- **Formateo eficiente**: Mejora la comprensi√≥n por LLMs

## Comandos Esenciales

### Iniciar vLLM
```bash
# phi4-mini
vllm serve microsoft/Phi-4-mini --host 0.0.0.0 --port 8001 --api-key EMPTY

# qwen2.5-coder-1.5b  
vllm serve Qwen/Qwen2.5-Coder-1.5B-Instruct --host 0.0.0.0 --port 8002 --api-key EMPTY

# gpt-oss-20b
vllm serve /home/elect/models/gpt-oss-20b --host 0.0.0.0 --port 8000 --api-key EMPTY
```

### Verificar Sistema
```bash
# Verificar conexi√≥n con vLLM
curl http://34.12.166.76:8000/v1/models

# Test simple
curl -X POST "http://34.12.166.76:8000/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer EMPTY" \
  -d '{
    "model": "phi4:mini",
    "messages": [{"role": "user", "content": "Hola"}],
    "max_tokens": 50
  }'
```

## Carpetas y Archivos Clave

### Directorios Principales
- `/home/elect/capibara6/` - Directorio principal
- `/home/elect/capibara6/vm-bounty2/` - Backend con modelos
- `/home/elect/capibara6/vm-rag3/` - Sistema RAG
- `/home/elect/models/` - Modelos f√≠sicos instalados

### Archivos de Configuraci√≥n 
- `model_config.json` - Configuraci√≥n principal de modelos
- `server.js` - API principal Node.js  
- `vm-bounty2/servers/consensus_server.py` - Servidor de consenso
- `backend/ollama_rag_integration.py` - Integraci√≥n RAG (renombrado a vLLM)

## Seguridad y Monitoreo

### Firewall
- Puertos 8000-8003 abiertos para vLLM
- IP interna RAG3 (10.154.0.2) autorizada para comunicaciones internas
- IP externa 34.12.166.76 para endpoints de servicios

### Monitoreo
- **Grafana**: http://10.154.0.2:3000 (m√©tricas del sistema)
- **Prometheus**: http://10.154.0.2:9090 (recolecci√≥n de m√©tricas)

## Estado Actual del Sistema ARM Axion

- ‚úÖ Migraci√≥n Ollama ‚Üí vLLM completada
- ‚úÖ Modelos phi3 ‚Üí phi4 y mistral ‚Üí qwen2.5-coder actualizados
- ‚úÖ Sistema de consenso con votaci√≥n ponderada operativo
- ‚úÖ Integraci√≥n RAG con E2B y TOON completamente funcional
- ‚úÖ Backend principal con enrutamiento sem√°ntico operativo
- ‚úÖ Frontend con chat responsive y plantillas integrado
- ‚úÖ Servicios MCP, TTS y otros completamente operativos

## üöÄ Integraci√≥n ARM Axion Reciente

**Nueva configuraci√≥n de 4 modelos incluyendo Gemma3:**

- ‚úÖ **phi4_fast**: Modelo r√°pido para tareas simples (AWQ)
- ‚úÖ **mistral_balanced**: Modelo equilibrado para tareas t√©cnicas (AWQ)
- ‚úÖ **qwen_coder**: Especializado en c√≥digo y programaci√≥n (AWQ)
- ‚úÖ **gemma3_multimodal**: Modelo de alta capacidad (bfloat16, 27B params)
- ‚úÖ **Optimizaciones ARM**: NEON/ACL activas y proporcionando beneficios
- ‚úÖ **Configuraci√≥n completa**: En `/home/elect/capibara6/arm-axion-optimizations/vllm_integration/`
- ‚ùå GPT-OSS-20B incompatible debido a arquitectura MoE personalizada

### Caracter√≠sticas de la integraci√≥n ARM Axion:
- **Puerto API**: 8080 (OpenAI-compatible)
- **Rendimiento Gemma3**: ~2.2 tokens/segundo en CPU
- **Memoria ajustada**: Par√°metros optimizados para manejar KV cache
- **Estabilidad**: Sistema operativo con 4 modelos concurrentes