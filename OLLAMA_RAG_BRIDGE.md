# üåâ Bridge Ollama-RAG: Integraci√≥n entre VMs

## Arquitectura de la Soluci√≥n

Esta soluci√≥n conecta dos VMs en Google Cloud para crear un sistema h√≠brido donde los modelos de Ollama pueden enriquecer sus respuestas con datos personales del usuario almacenados en el sistema RAG.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    VM bounty2 (europe-west4-a)              ‚îÇ
‚îÇ  IP Interna: 10.164.0.9      IP P√∫blica: 34.12.166.76      ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
‚îÇ  ‚îÇ   Ollama     ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ  Servidor Python   ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ  :11434      ‚îÇ         ‚îÇ  capibara6_server  ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ         ‚îÇ  :5001             ‚îÇ           ‚îÇ
‚îÇ  ‚îÇ  Models:     ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
‚îÇ  ‚îÇ  - mistral   ‚îÇ                  ‚îÇ                        ‚îÇ
‚îÇ  ‚îÇ  - phi3:mini ‚îÇ                  ‚îÇ RAGClient             ‚îÇ
‚îÇ  ‚îÇ  - gpt-oss   ‚îÇ                  ‚îÇ consulta datos        ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ                        ‚îÇ
‚îÇ                                     ‚îÇ                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      ‚îÇ
                                      ‚îÇ HTTP Request
                                      ‚îÇ (10.154.0.2:8000)
                                      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     VM RAG3 (europe-west2-c)                ‚îÇ
‚îÇ  IP Interna: 10.154.0.2      IP P√∫blica: 34.105.131.8      ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
‚îÇ  ‚îÇ           Sistema RAG Completo                 ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ                                                 ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Milvus   ‚îÇ  ‚îÇ PostgreSQL‚îÇ  ‚îÇ  Nebula    ‚îÇ  ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Vector DB‚îÇ  ‚îÇ  Relacional‚îÇ  ‚îÇ  Graph DB  ‚îÇ  ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ :19530   ‚îÇ  ‚îÇ  :5432    ‚îÇ  ‚îÇ  :9669     ‚îÇ  ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ                                                 ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  API Server (FastAPI) :8000                    ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  - /api/search/semantic                        ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  - /api/search/rag                             ‚îÇ         ‚îÇ
‚îÇ  ‚îÇ  - /api/search/all                             ‚îÇ         ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
‚îÇ                                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Componentes Implementados

### 1. **RAGClient** (`backend/rag_client.py`)

Cliente HTTP para consultar el sistema RAG desde bounty2.

**Caracter√≠sticas:**
- ‚úÖ Conexi√≥n a trav√©s de red privada de GCloud
- ‚úÖ Reintentos autom√°ticos en caso de fallo
- ‚úÖ Timeout configurable
- ‚úÖ M√∫ltiples m√©todos de b√∫squeda:
  - `search_semantic()`: B√∫squeda vectorial
  - `search_rag()`: B√∫squeda completa (Vector + SQL + Grafo)
  - `search_all_collections()`: B√∫squeda multi-colecci√≥n
  - `get_context_for_llm()`: Contexto formateado para LLMs

**Uso b√°sico:**
```python
from rag_client import RAGClient

# Inicializar cliente
rag_client = RAGClient(base_url="http://10.154.0.2:8000")

# Obtener contexto para enriquecer respuesta
context = rag_client.get_context_for_llm(
    user_query="¬øQu√© he hablado sobre IA?",
    n_results=3
)
```

### 2. **OllamaRAGIntegration** (`backend/ollama_rag_integration.py`)

Capa de integraci√≥n que decide autom√°ticamente cu√°ndo usar RAG.

**Caracter√≠sticas:**
- ‚úÖ Detecci√≥n inteligente de consultas personales
- ‚úÖ Enriquecimiento autom√°tico de prompts
- ‚úÖ Soporte para streaming
- ‚úÖ Fallback a Ollama puro si RAG falla
- ‚úÖ M√©tricas de uso de RAG

**Detecci√≥n de Consultas Personales:**

El sistema detecta autom√°ticamente cuando una consulta requiere datos personales usando patrones regex:

- Referencias personales: "mi", "mis", "yo", "he", "tengo"
- Referencias a conversaciones: "dije", "habl√©", "coment√©"
- Referencias a archivos: "guard√©", "archivo", "documento"
- Preguntas sobre datos: "qu√© tengo", "qu√© dije"

**Uso b√°sico:**
```python
from ollama_rag_integration import create_integrated_client
import json

# Cargar config de Ollama
with open("model_config.json") as f:
    ollama_config = json.load(f)

# Crear cliente integrado
client = create_integrated_client(
    ollama_config=ollama_config,
    rag_url="http://10.154.0.2:8000"
)

# Generar respuesta (usa RAG autom√°ticamente si es necesario)
response = client.generate_with_rag(
    prompt="¬øQu√© he comentado sobre machine learning?",
    model_tier="balanced"
)

print(f"RAG usado: {response['rag_used']}")
print(f"Respuesta: {response['response']}")
```

---

## Configuraci√≥n en bounty2

### Paso 1: Instalar Dependencias

```bash
cd /home/elect/capibara6/backend
pip3 install requests urllib3
```

### Paso 2: Configurar Variables de Entorno

Crear o actualizar `.env`:

```bash
# URL del servidor RAG en RAG3
RAG_API_URL=http://10.154.0.2:8000

# Configuraci√≥n de Ollama (local en bounty2)
OLLAMA_ENDPOINT=http://localhost:11434
DEFAULT_MODEL_TIER=balanced
```

### Paso 3: Modificar Servidor Existente

Actualizar `backend/capibara6_integrated_server.py` o tu servidor actual:

```python
from ollama_rag_integration import create_integrated_client
import json
import os

# Cargar configuraci√≥n
with open("model_config.json") as f:
    ollama_config = json.load(f)

# Crear cliente integrado
integrated_client = create_integrated_client(
    ollama_config=ollama_config,
    rag_url=os.getenv("RAG_API_URL", "http://10.154.0.2:8000")
)

# En tu endpoint de chat
@app.post("/api/chat")
def chat(request):
    user_message = request.json.get("message")

    # Usar cliente integrado (usa RAG autom√°ticamente si es necesario)
    response = integrated_client.generate_with_rag(
        prompt=user_message,
        model_tier="auto",  # Selecci√≥n autom√°tica de modelo
        use_rag=True
    )

    return {
        "response": response["response"],
        "model": response["model"],
        "rag_used": response["rag_used"],
        "metadata": response.get("rag_metadata", {})
    }
```

---

## Testing y Verificaci√≥n

### Test 1: Verificar Conectividad

Desde bounty2:

```bash
# Verificar que RAG3 es accesible
ping -c 2 10.154.0.2

# Test health check del API RAG
curl -s http://10.154.0.2:8000/health | python3 -m json.tool
```

### Test 2: Cliente RAG

```bash
cd /home/elect/capibara6/backend

# Ejecutar demo del cliente
python3 rag_client.py
```

Salida esperada:
```
=== Health Check ===
{'status': 'healthy', 'services': {...}}

=== B√∫squeda RAG ===
Query: machine learning
Context length: 450
Sources: 2
```

### Test 3: Integraci√≥n Completa

```bash
# Ejecutar demo de integraci√≥n
python3 ollama_rag_integration.py
```

Salida esperada:
```
=== Test 1: Pregunta general ===
RAG usado: False
Respuesta: Machine learning es una rama...

=== Test 2: Pregunta personal ===
RAG usado: True
RAG confidence: 0.60
Respuesta: Bas√°ndome en tus conversaciones anteriores...
```

---

## Flujo de una Consulta

### Caso 1: Pregunta General (Sin RAG)

```
Usuario: "¬øQu√© es machine learning?"
    ‚Üì
OllamaRAGIntegration.should_use_rag() ‚Üí False
    ‚Üì
Prompt enviado directamente a Ollama
    ‚Üì
Respuesta general de Ollama
```

### Caso 2: Pregunta Personal (Con RAG)

```
Usuario: "¬øQu√© he hablado sobre machine learning?"
    ‚Üì
OllamaRAGIntegration.should_use_rag() ‚Üí True (confidence: 0.6)
    ‚Üì
RAGClient.search_rag("machine learning") ‚Üí Contexto
    ‚Üì
Prompt enriquecido = Contexto + Pregunta original
    ‚Üì
Prompt enviado a Ollama
    ‚Üì
Respuesta personalizada basada en datos del usuario
```

---

## M√©tricas y Monitoreo

### M√©tricas Disponibles

Cada respuesta incluye metadata:

```json
{
  "response": "...",
  "model": "mistral:latest",
  "rag_used": true,
  "rag_metadata": {
    "used_rag": true,
    "confidence": 0.60,
    "context_length": 1234
  }
}
```

### Logs

El sistema logea autom√°ticamente:
- Decisiones de uso de RAG
- Errores de conectividad
- Tiempos de respuesta
- Contextos utilizados

---

## Optimizaciones Futuras

### 1. Cache de Contextos RAG
```python
from functools import lru_cache

@lru_cache(maxsize=100)
def get_cached_rag_context(query: str) -> str:
    return rag_client.get_context_for_llm(query)
```

### 2. B√∫squeda Paralela

Consultar m√∫ltiples fuentes en paralelo:

```python
import asyncio

async def parallel_rag_search(query: str):
    tasks = [
        search_semantic(query),
        search_graph(query),
        search_sql(query)
    ]
    results = await asyncio.gather(*tasks)
    return combine_results(results)
```

### 3. Embeddings Pre-computados

Para consultas frecuentes, precomputar embeddings:

```python
# En RAG3
embeddings_cache = {
    "machine learning": [0.1, 0.3, ...],
    "IA conversacional": [0.2, 0.4, ...]
}
```

---

## Troubleshooting

### Error: "Connection refused"

**Problema:** No se puede conectar a RAG3

**Soluci√≥n:**
```bash
# Verificar que RAG3 est√° ejecutando el API
ssh rag3
docker ps | grep capibara6-api

# Verificar firewall
gcloud compute firewall-rules list | grep rag3
```

### Error: "Empty RAG context"

**Problema:** RAG no encuentra datos relevantes

**Soluci√≥n:**
1. Verificar que hay datos en las colecciones
2. Reducir el umbral de similitud
3. Usar b√∫squeda m√°s amplia

```python
# B√∫squeda m√°s permisiva
rag_client.search_rag(query, n_results=10, use_graph=True)
```

### Latencia Alta

**Problema:** Respuestas lentas

**Soluci√≥n:**
1. Reducir `n_results`
2. Deshabilitar b√∫squeda de grafo para consultas simples
3. Implementar cache

---

## Seguridad

### Red Privada

Las VMs se comunican a trav√©s de red privada de GCloud:
- ‚úÖ No expuesto a Internet
- ‚úÖ Autenticaci√≥n por firewall de GCloud
- ‚úÖ Encriptaci√≥n en tr√°nsito

### Autenticaci√≥n (Opcional)

Para agregar autenticaci√≥n:

```python
class RAGClient:
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key
        self.session.headers.update({
            "X-API-Key": api_key
        })
```

---

## Conclusi√≥n

Este bridge permite que:

‚úÖ **Ollama** acceda a datos personales del usuario sin necesidad de moverlos
‚úÖ **RAG3** mantenga todos los datos centralizados y seguros
‚úÖ **bounty2** aproveche el sistema RAG existente sin duplicar infraestructura
‚úÖ **Respuestas personalizadas** basadas en el historial del usuario

**Latencia:** ~9ms entre VMs (excelente)
**Disponibilidad:** 99.9% (red privada de GCloud)
**Escalabilidad:** Horizontal en ambas VMs independientemente

---

*Generado autom√°ticamente por Claude Code*
*Fecha: 2025-11-11*
