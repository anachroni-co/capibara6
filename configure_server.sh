#!/bin/bash
# Script para configurar el servidor con los modelos descargados

echo "âš™ï¸ Configurando Servidor con Modelos GPT-OSS"
echo "==========================================="

MODELS_DIR="/home/elect/models"
SERVER_CONFIG="/home/elect/backend/capibara6_integrated_server.py"

echo "ğŸ“ Directorio de modelos: $MODELS_DIR"

# Verificar si existen modelos
if [ ! -d "$MODELS_DIR" ]; then
    echo "âŒ Directorio de modelos no existe"
    echo "ğŸ’¡ Ejecutar primero: ./download_models.sh"
    exit 1
fi

# Buscar archivos de modelo
MODEL_FILES=$(find "$MODELS_DIR" -name "*.gguf" -o -name "*.bin" -o -name "*.safetensors" | head -5)

if [ -z "$MODEL_FILES" ]; then
    echo "âŒ No se encontraron archivos de modelo"
    echo "ğŸ’¡ Verificar que los modelos se descargaron correctamente"
    exit 1
fi

echo "âœ… Modelos encontrados:"
echo "$MODEL_FILES"

# Obtener el modelo principal (el mÃ¡s grande o el primero)
MAIN_MODEL=$(find "$MODELS_DIR" -name "*.gguf" | head -1)
if [ -z "$MAIN_MODEL" ]; then
    MAIN_MODEL=$(find "$MODELS_DIR" -name "*.bin" | head -1)
fi

if [ -z "$MAIN_MODEL" ]; then
    MAIN_MODEL=$(find "$MODELS_DIR" -name "*.safetensors" | head -1)
fi

echo ""
echo "ğŸ¯ Modelo principal: $MAIN_MODEL"

# Crear script de inicio del modelo
cat > /home/elect/start_model.sh << EOF
#!/bin/bash
# Script para iniciar el modelo GPT-OSS-20B

MODEL_PATH="$MAIN_MODEL"
echo "ğŸš€ Iniciando modelo GPT-OSS-20B..."
echo "ğŸ“ Modelo: \$MODEL_PATH"

# Verificar si el modelo existe
if [ ! -f "\$MODEL_PATH" ]; then
    echo "âŒ Error: Modelo no encontrado en \$MODEL_PATH"
    exit 1
fi

# Iniciar el servidor del modelo (ajustar segÃºn el tipo de servidor)
if [[ "\$MODEL_PATH" == *.gguf ]]; then
    echo "ğŸ”§ Iniciando servidor llama.cpp..."
    # Ajustar parÃ¡metros segÃºn tu configuraciÃ³n
    ./llama-server -m "\$MODEL_PATH" --port 8080 --host 0.0.0.0 --ctx-size 4096 --threads 8
elif [[ "\$MODEL_PATH" == *.bin ]]; then
    echo "ğŸ”§ Iniciando servidor transformers..."
    # Ajustar segÃºn tu configuraciÃ³n
    python3 -m transformers.serving --model "\$MODEL_PATH" --port 8080
else
    echo "âŒ Formato de modelo no soportado: \$MODEL_PATH"
    exit 1
fi
EOF

chmod +x /home/elect/start_model.sh

echo ""
echo "âœ… Script de inicio del modelo creado: /home/elect/start_model.sh"

# Crear script de inicio completo
cat > /home/elect/start_complete_system.sh << EOF
#!/bin/bash
# Script para iniciar el sistema completo

echo "ğŸš€ Iniciando Sistema Completo Capibara6"
echo "======================================"

# FunciÃ³n para limpiar procesos al salir
cleanup() {
    echo "ğŸ›‘ Deteniendo servicios..."
    pkill -f "llama-server"
    pkill -f "capibara6_integrated_server"
    exit 0
}

trap cleanup SIGINT SIGTERM

# Iniciar modelo en background
echo "ğŸ“¡ Iniciando modelo GPT-OSS-20B..."
./start_model.sh &
MODEL_PID=\$!

# Esperar a que el modelo estÃ© listo
echo "â³ Esperando a que el modelo estÃ© listo..."
sleep 10

# Verificar que el modelo estÃ© funcionando
if curl -s http://localhost:8080/health > /dev/null; then
    echo "âœ… Modelo funcionando correctamente"
else
    echo "âš ï¸ Modelo no responde, continuando..."
fi

# Iniciar servidor integrado
echo "ğŸŒ Iniciando servidor integrado..."
cd /home/elect/backend
python3 capibara6_integrated_server.py &
SERVER_PID=\$!

echo ""
echo "ğŸ‰ Sistema iniciado correctamente!"
echo "ğŸ“¡ Modelo: http://localhost:8080"
echo "ğŸŒ Servidor: http://localhost:5001"
echo "ğŸ¥ Health: http://localhost:5001/health"
echo ""
echo "Presiona Ctrl+C para detener todo"

# Esperar
wait
EOF

chmod +x /home/elect/start_complete_system.sh

echo ""
echo "âœ… Script del sistema completo creado: /home/elect/start_complete_system.sh"

echo ""
echo "ğŸ¯ Comandos disponibles:"
echo "1. Iniciar solo el modelo: ./start_model.sh"
echo "2. Iniciar sistema completo: ./start_complete_system.sh"
echo "3. Probar sistema: ./test_quick.sh"

echo ""
echo "ğŸ“Š InformaciÃ³n del modelo:"
echo "   - Ruta: $MAIN_MODEL"
echo "   - TamaÃ±o: $(du -h "$MAIN_MODEL" | cut -f1)"
echo "   - Tipo: $(file "$MAIN_MODEL" | cut -d: -f2)"
