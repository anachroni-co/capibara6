# IntegraciÃ³n BB + Capibara6

Arquitectura de dos servidores para separar modelos AI de servicios auxiliares.

## ğŸ—ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          FRONTEND                                â”‚
â”‚                      (Vercel / Web)                              â”‚
â”‚                   https://capibara6.com                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â”‚ HTTPS
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   CAPIBARA6 - Backend Servicios                  â”‚
â”‚                        VM 2: 34.175.215.109                      â”‚
â”‚                           Puerto 5001                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Servidor Integrado (capibara6_integrated_server.py)            â”‚
â”‚  â”œâ”€ Semantic Router     â†’ SelecciÃ³n automÃ¡tica de modelo        â”‚
â”‚  â”œâ”€ Smart MCP          â†’ Contexto inteligente                   â”‚
â”‚  â”œâ”€ Coqui TTS          â†’ SÃ­ntesis de voz                        â”‚
â”‚  â””â”€ Consensus Server   â†’ Manejo multi-modelo                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â”‚ HTTP
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      BB - Backend Modelos                        â”‚
â”‚                   VM 1: 34.175.215.109 (?)                       â”‚
â”‚                    Repositorio: gmarko/BB                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Modelos AI (llama-server / vllm)                               â”‚
â”‚  â”œâ”€ GPT-OSS-20B    â†’ Puerto 8080  (20B params)                  â”‚
â”‚  â”œâ”€ Phi-Mini       â†’ Puerto 8081  (3.8B params)                 â”‚
â”‚  â””â”€ Mixtral 8x7B   â†’ Puerto 8082  (~47B params)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ SeparaciÃ³n de Responsabilidades

### **Capibara6 (Backend Servicios)**
**Responsabilidad**: OrquestaciÃ³n, servicios auxiliares y lÃ³gica de negocio

**Componentes**:
- âœ… **Semantic Router** - Selecciona quÃ© modelo usar segÃºn la query
- âœ… **Smart MCP** - AÃ±ade contexto inteligente a las queries
- âœ… **TOON Format** - OptimizaciÃ³n de tokens (30-60% de ahorro vs JSON)
- âœ… **Coqui TTS** - Convierte texto a voz
- âœ… **Consensus Server** - Combina respuestas de mÃºltiples modelos
- âœ… **API Proxies** - Maneja CORS y enrutamiento

**Puerto**: 5001
**Repositorio**: anachroni-co/capibara6

---

### **BB (Backend Modelos)**
**Responsabilidad**: Servir modelos de lenguaje

**Modelos disponibles**:

| Modelo | Puerto | ParÃ¡metros | Uso Principal |
|--------|--------|------------|---------------|
| **gpt-oss-20b** | 8080 | 20B | ProgramaciÃ³n, MatemÃ¡ticas, AnÃ¡lisis |
| **phi-mini** | 8081 | 3.8B | Facts rÃ¡pidos, ConversaciÃ³n |
| **mixtral** | 8082 | ~47B | Creatividad, TraducciÃ³n |

**Repositorio**: gmarko/BB
**Servidor**: llama-server, vllm o similar

---

## ğŸ”„ Flujo de Request

### 1. Usuario hace una query

```
Usuario: "cÃ³mo programar en Python"
   â†“
Frontend (Vercel)
   â†“ POST /api/chat
Capibara6 Backend (5001)
```

### 2. Semantic Router selecciona modelo

```python
# En capibara6_integrated_server.py
routing_decision = semantic_router.select_model("cÃ³mo programar en Python")

# Resultado:
{
    "model_id": "gpt-oss-20b",
    "route_name": "programming",
    "confidence": 0.9,
    "reasoning": "Query clasificada como 'programming'"
}
```

### 3. Smart MCP aÃ±ade contexto

```python
enhanced_message = smart_mcp.enhance_message_with_context(query)
# AÃ±ade fecha actual, identidad del bot, etc. si es relevante
```

### 4. Request al modelo en BB

```
Capibara6 Backend (5001)
   â†“ POST http://34.175.215.109:8080/completion
BB - gpt-oss-20b (8080)
   â†“ Respuesta generada
Capibara6 Backend (5001)
   â†“ Response JSON
Frontend
   â†“ Display
Usuario
```

---

## ğŸ“Š ConfiguraciÃ³n Actual

### Modelos Activos (en models_config.py)

```python
MODELS_CONFIG = {
    'gpt-oss-20b': {
        'name': 'GPT-OSS-20B',
        'server_url': 'http://34.175.215.109:8080/completion',
        'hardware': 'GPU',
        'status': 'active',
    },

    'phi': {
        'name': 'Phi-3 Mini',
        'server_url': 'http://34.175.215.109:8081/completion',
        'hardware': 'GPU',
        'status': 'active',
    },

    'mixtral': {
        'name': 'Mixtral 8x7B',
        'server_url': 'http://34.175.215.109:8082/completion',
        'hardware': 'GPU',
        'status': 'active',
    }
}
```

### Routing de Semantic Router

```python
model_mapping = {
    "programming": "gpt-oss-20b",      # CÃ³digo, debugging
    "creative_writing": "mixtral",      # Cuentos, poemas
    "quick_facts": "phi",               # Definiciones rÃ¡pidas
    "analysis": "gpt-oss-20b",          # AnÃ¡lisis profundo
    "conversation": "phi",              # Chat casual
    "math": "gpt-oss-20b",              # MatemÃ¡ticas
    "translation": "mixtral",           # TraducciÃ³n
    "default": "gpt-oss-20b"            # Fallback
}
```

---

## ğŸš€ Deployment

### **Capibara6 (Backend Servicios)**

```bash
# En VM 2
cd capibara6/backend
pip install -r requirements.txt
python capibara6_integrated_server.py

# Se inicia en puerto 5001
```

**Health check**:
```bash
curl http://localhost:5001/health
```

---

### **BB (Backend Modelos)**

```bash
# En VM 1 (configuraciÃ³n depende del repo BB)
cd BB

# Ejemplo con llama-server
llama-server --model gpt-oss-20b.gguf --port 8080 &
llama-server --model phi-mini.gguf --port 8081 &
llama-server --model mixtral-8x7b.gguf --port 8082 &
```

**Health check**:
```bash
curl http://localhost:8080/health
curl http://localhost:8081/health
curl http://localhost:8082/health
```

---

## ğŸ”§ ConfiguraciÃ³n de Puertos

### VM 1 - BB (Modelos)
- **8080**: gpt-oss-20b
- **8081**: phi-mini
- **8082**: mixtral

### VM 2 - Capibara6 (Servicios)
- **5001**: Servidor integrado principal
- **5002**: Consensus server (âš ï¸ conflicto con TTS)
- **5003**: Smart MCP standalone
- **5010**: Smart MCP alternativo

---

## ğŸ”Œ API Endpoints

### Capibara6 â†’ BB

**Request a modelo**:
```bash
POST http://34.175.215.109:8080/completion
Content-Type: application/json

{
  "prompt": "texto mejorado con contexto",
  "n_predict": 200,
  "temperature": 0.7,
  "top_p": 0.9,
  "repeat_penalty": 1.2,
  "stream": true
}
```

**Response del modelo**:
```json
{
  "content": "respuesta generada",
  "tokens_predicted": 150,
  "tokens_evaluated": 50
}
```

---

## ğŸ“ Testing

### Test del Semantic Router

```bash
cd backend
python test_semantic_router.py

# Output mostrarÃ¡ quÃ© modelo se selecciona para cada query
```

### Test de integraciÃ³n completa

```bash
# Test routing
curl -X POST http://localhost:5001/api/router/test \
  -H "Content-Type: application/json" \
  -d '{"query": "cÃ³mo programar en Python"}'

# Test chat completo
curl -X POST http://localhost:5001/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "escribe un cuento sobre robots"}'
```

---

## ğŸ› Troubleshooting

### Error: "Error de conexiÃ³n con modelo"

**Causa**: BB no estÃ¡ respondiendo en el puerto esperado

**SoluciÃ³n**:
```bash
# Verificar que los modelos estÃ©n corriendo
lsof -i :8080
lsof -i :8081
lsof -i :8082

# Ver logs de BB
tail -f /var/log/bb/llama-server.log
```

### Error: "Modelo no configurado"

**Causa**: El Semantic Router seleccionÃ³ un modelo que no existe en models_config.py

**SoluciÃ³n**:
1. Verificar que todos los modelos en `semantic_model_router.py` existan en `models_config.py`
2. Verificar que el `status` sea `'active'`

### Performance lento

**Posibles causas**:
- Modelos grandes en CPU en vez de GPU
- MÃºltiples requests simultÃ¡neos sin balanceo
- Contexto muy largo

**Soluciones**:
- Verificar que BB use GPU: `nvidia-smi`
- Implementar cola de requests
- Limitar tokens de contexto

---

## ğŸ”’ Seguridad

### Firewall
```bash
# Solo permitir conexiones desde Capibara6 a BB
# En VM de BB:
sudo ufw allow from 34.175.215.109 to any port 8080
sudo ufw allow from 34.175.215.109 to any port 8081
sudo ufw allow from 34.175.215.109 to any port 8082
```

### Rate Limiting
Implementar en Capibara6 para evitar abuse:
```python
from flask_limiter import Limiter

limiter = Limiter(app, key_func=get_remote_address)

@app.route('/api/chat')
@limiter.limit("10 per minute")
def chat():
    # ...
```

---

## ğŸ“ˆ Monitoreo

### MÃ©tricas clave

**Capibara6**:
- Requests por minuto
- Latencia de routing (ms)
- Errores de conexiÃ³n a BB

**BB**:
- Tokens por segundo
- Uso de VRAM/RAM
- Queue length
- Tiempo de generaciÃ³n

### Logs

**Capibara6**:
```bash
tail -f backend/logs/capibara6.log
```

**BB**:
```bash
# Depende de configuraciÃ³n de BB
tail -f /var/log/bb/*.log
```

---

## ğŸ”„ PrÃ³ximos Pasos

- [ ] Confirmar IPs exactas de VMs
- [ ] Obtener acceso al repositorio BB
- [ ] Documentar configuraciÃ³n exacta de BB
- [ ] Implementar health checks automÃ¡ticos
- [ ] Agregar failover si un modelo cae
- [ ] Implementar cachÃ© de respuestas
- [ ] Agregar mÃ©tricas con Prometheus

---

## ğŸ“š Referencias

- Repositorio BB: https://github.com/gmarko/BB (privado)
- Repositorio Capibara6: https://github.com/anachroni-co/capibara6
- Semantic Router: `backend/SEMANTIC_ROUTER_README.md`
- TOON Format: `TOON_GUIDE.md`
- Models Config: `backend/models_config.py`

---

**Ãšltima actualizaciÃ³n**: Noviembre 2025
**VersiÃ³n**: 1.0.0
